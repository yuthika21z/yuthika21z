{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuthika21z/yuthika21z/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrfwu8G4J3Ws"
      },
      "source": [
        "**Sentiment Analysis of IMDB Movie Reviews**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELiX6NuSRlex",
        "outputId": "d47d79b5-000e-4500-ecf7-66903607a821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpng9J3eJ3Wv"
      },
      "source": [
        "**Problem Statement:**\n",
        "\n",
        "In this, we have to predict the number of positive and negative reviews based on sentiments by using different classification models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL2SJVCpJ3Ww"
      },
      "source": [
        "**Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_z88nGHjJ3Ww"
      },
      "outputs": [],
      "source": [
        "#Load the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPWEW00iJ3Wy"
      },
      "source": [
        "**Import the training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0Bpsa9_J3Wy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#importing the training data\n",
        "imdb_data=pd.read_csv('/content/drive/MyDrive/Academics 2022-23/Deep Learning Study material/Unit4/IMDB Dataset.csv')\n",
        "print(imdb_data.shape)\n",
        "imdb_data.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-boO4g7J3Wz"
      },
      "source": [
        "**Exploratery data analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "UgrPzxwYJ3Wz",
        "outputId": "827600fd-fd2c-4034-e851-4da8e1be209d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-78758cfe-b368-4dc6-9a90-3bcb2367dd3e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78758cfe-b368-4dc6-9a90-3bcb2367dd3e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78758cfe-b368-4dc6-9a90-3bcb2367dd3e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78758cfe-b368-4dc6-9a90-3bcb2367dd3e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  positive\n",
              "freq                                                    5     25000"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Summary of the dataset\n",
        "imdb_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFlAkvZUJ3W0"
      },
      "source": [
        "**Sentiment count**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VobFLD2JJ3W0",
        "outputId": "fc57861e-ae69-4c7c-8637-5b85e23412f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "positive    25000\n",
              "negative    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#sentiment count\n",
        "imdb_data['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdNFtxlaJ3W0"
      },
      "source": [
        "We can see that the dataset is balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbpckDK_J3W1"
      },
      "source": [
        "**Spliting the training dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCDXtNLdJ3W1",
        "outputId": "3bf635cd-dd70-489c-86e7-7f483389389b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40000,) (40000,)\n",
            "(10000,) (10000,)\n"
          ]
        }
      ],
      "source": [
        "#split the dataset\n",
        "#train dataset\n",
        "train_reviews=imdb_data.review[:40000]\n",
        "train_sentiments=imdb_data.sentiment[:40000]\n",
        "#test dataset\n",
        "test_reviews=imdb_data.review[40000:]\n",
        "test_sentiments=imdb_data.sentiment[40000:]\n",
        "print(train_reviews.shape,train_sentiments.shape)\n",
        "print(test_reviews.shape,test_sentiments.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckSQd-SnJ3W1"
      },
      "source": [
        "**Text normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9P_yfH6J3W2",
        "outputId": "8835ac84-cea3-4a24-f598-d2b99e45d882"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Tokenization of text\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhTQuxTjSoKa"
      },
      "outputs": [],
      "source": [
        "stopword_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5ofXiy6J3W2"
      },
      "source": [
        "**Removing html strips and noise text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8IBXOPeJ3W2",
        "outputId": "a7ee2089-a227-4fb8-99ea-d74b1dc40e5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-44807a6118ae>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ],
      "source": [
        "#Removing the html strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "imdb_data['review']=imdb_data['review'].apply(denoise_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTw_N29SJ3W2"
      },
      "source": [
        "**Removing special characters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd-XANqeJ3W3"
      },
      "outputs": [],
      "source": [
        "#Define function for removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2nMUOFSJ3W3"
      },
      "source": [
        "**Text stemming\n",
        "**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx6n8vz6J3W3"
      },
      "outputs": [],
      "source": [
        "#Stemming the text\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "#Apply function on review column\n",
        "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zk0LNHEJ3W3"
      },
      "source": [
        "**Removing stopwords**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7L5MsNsJ3W3",
        "outputId": "f11df043-bc02-4d45-dc38-061e558ef758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'why', 'our', 'will', 'further', 'just', 'there', 'i', 'some', 'but', 'don', 'didn', 'on', \"wasn't\", \"you're\", 'too', 'other', 'yourself', 'were', \"she's\", 'after', 'before', 'what', 'doing', 'its', 'then', 'do', 'him', 'needn', 'has', 're', 'ain', 'we', 'and', 'was', 'doesn', 'be', 'ma', 'you', 'down', 'in', 'he', 'with', 'wouldn', 'theirs', 'when', 'my', \"that'll\", 'couldn', 'such', 'while', 'under', 'nor', 't', 'itself', 'into', 'had', 'their', 'been', 'against', 'above', \"don't\", 'haven', 'if', 'here', 'up', 'out', 'is', 'own', 'it', 'himself', 'again', \"shan't\", 'during', 'shan', 'are', 'ours', 'about', \"needn't\", 'his', 'those', \"you'd\", 'all', 'yourselves', 'these', 'each', 'does', 'am', 'can', 'your', 'to', 'should', \"hadn't\", 'which', 'than', \"should've\", 'd', 'below', 'hasn', 'aren', \"aren't\", 'until', 'a', \"doesn't\", 'from', 'y', 'won', 'them', \"you'll\", 'me', 'ourselves', \"weren't\", 'o', 'did', \"wouldn't\", 'themselves', 'whom', 'both', 'only', \"hasn't\", 'that', 'hers', 'as', 'for', \"mustn't\", \"you've\", 'yours', 'most', 'myself', 'this', 'll', 'once', 'wasn', 'by', \"mightn't\", 'more', 'at', 'off', 'they', \"shouldn't\", 'weren', 'no', 'few', 'through', 'mustn', 'mightn', 'who', 'between', 'm', 'an', \"it's\", \"won't\", 'having', 'same', 'now', \"isn't\", 'isn', 'she', 'any', 've', 'so', 'over', 'the', 'or', 'of', 'very', 'how', 's', \"haven't\", 'where', 'hadn', 'her', 'being', 'because', 'have', \"couldn't\", 'not', 'shouldn', 'herself', \"didn't\"}\n"
          ]
        }
      ],
      "source": [
        "#set stopwords to english\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3YG60WzJ3W4"
      },
      "source": [
        "**Normalized train reviews**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "4dr0NyVDJ3W4",
        "outputId": "a08cd252-b7a9-4aed-f02d-f0741fd2c08d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'One reviewers mentioned watching 1 Oz episode youll hooked right exactly happened meThe first thing struck Oz brutality unflinching scenes violence set right word GO Trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordIt called OZ nickname given Oswald Maximum Security State Penitentary focuses mainly Emerald City experimental section prison cells glass fronts face inwards privacy high agenda Em City home manyAryans Muslims gangstas Latinos Christians Italians Irish moreso scuffles death stares dodgy dealings shady agreements never far awayI would say main appeal show due fact goes shows wouldnt dare Forget pretty pictures painted mainstream audiences forget charm forget romanceOZ doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste Oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience Watching Oz may become comfortable uncomfortable viewingthats get touch darker side'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#normalized train reviews\n",
        "norm_train_reviews=imdb_data.review[:40000]\n",
        "norm_train_reviews[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAzOWa6nJ3W4"
      },
      "source": [
        "**Normalized test reviews**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAS7InizJ3W4"
      },
      "outputs": [],
      "source": [
        "#Normalized test reviews\n",
        "norm_test_reviews=imdb_data.review[40000:]\n",
        "norm_test_reviews[45005]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D64U0mNmJ3W5"
      },
      "source": [
        "**Bags of words model **\n",
        "\n",
        "It is used to convert text documents to numerical vectors or bag of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M41PuWIRJ3W5",
        "outputId": "2d29ff53-c8cd-411d-d61c-4af2c6d47cb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOW_cv_train: (40000, 6675873)\n",
            "BOW_cv_test: (10000, 6675873)\n"
          ]
        }
      ],
      "source": [
        "#Count vectorizer for bag of words\n",
        "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
        "#transformed train reviews\n",
        "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
        "#transformed test reviews\n",
        "cv_test_reviews=cv.transform(norm_test_reviews)\n",
        "\n",
        "print('BOW_cv_train:',cv_train_reviews.shape)\n",
        "print('BOW_cv_test:',cv_test_reviews.shape)\n",
        "#vocab=cv.get_feature_names()-toget feature names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGwEiIBceh4i"
      },
      "outputs": [],
      "source": [
        "cv_train_reviews[0,0:90]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWXlAvMYJ3W5"
      },
      "source": [
        "**Term Frequency-Inverse Document Frequency model (TFIDF)**\n",
        "\n",
        "It is used to convert text documents to  matrix of  tfidf features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi-E2VF6J3W5"
      },
      "outputs": [],
      "source": [
        "#Tfidf vectorizer\n",
        "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
        "#transformed train reviews\n",
        "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
        "#transformed test reviews\n",
        "tv_test_reviews=tv.transform(norm_test_reviews)\n",
        "print('Tfidf_train:',tv_train_reviews.shape)\n",
        "print('Tfidf_test:',tv_test_reviews.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KboEm2ZDJ3W5"
      },
      "source": [
        "**Labeling the sentiment text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYq8v4d_J3W6",
        "outputId": "2f7fe483-5f72-4cd6-c077-fd4153c5b5a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 1)\n"
          ]
        }
      ],
      "source": [
        "#labeling the sentient data\n",
        "lb=LabelBinarizer()\n",
        "#transformed sentiment data\n",
        "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
        "print(sentiment_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fzDaK3OJ3W6"
      },
      "source": [
        "**Split the sentiment tdata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfLtxhY0J3W6",
        "outputId": "dea2c480-e1bc-4c6b-c019-6b270665f404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ],
      "source": [
        "#Spliting the sentiment data\n",
        "train_sentiments=sentiment_data[:40000]\n",
        "test_sentiments=sentiment_data[40000:]\n",
        "print(train_sentiments)\n",
        "print(test_sentiments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK8J5l8FJ3W6"
      },
      "source": [
        "**Modelling the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onAWB843J3W6"
      },
      "source": [
        "Let us build logistic regression model for both bag of words and tfidf features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEVC7xEaJ3W6",
        "outputId": "3afce631-a434-4c54-a469-943abd58c789"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "#training the model\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
        "#Fitting the model for Bag of words\n",
        "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
        "print(lr_bow)\n",
        "#Fitting the model for tfidf features\n",
        "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
        "print(lr_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFhZWR9aJ3W6"
      },
      "source": [
        "**Logistic regression model performane on test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J_7pirmJ3XH"
      },
      "outputs": [],
      "source": [
        "#Predicting the model for bag of words\n",
        "lr_bow_predict=lr.predict(cv_test_reviews)\n",
        "print(lr_bow_predict)\n",
        "##Predicting the model for tfidf features\n",
        "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
        "print(lr_tfidf_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS1EzQi6J3XH"
      },
      "source": [
        "**Accuracy of the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAnF-2u3J3XI"
      },
      "outputs": [],
      "source": [
        "#Accuracy score for bag of words\n",
        "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
        "print(\"lr_bow_score :\",lr_bow_score)\n",
        "#Accuracy score for tfidf features\n",
        "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
        "print(\"lr_tfidf_score :\",lr_tfidf_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsjbGamZJ3XI"
      },
      "source": [
        "**Print the classification report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaAb7VsbJ3XJ"
      },
      "outputs": [],
      "source": [
        "#Classification report for bag of words\n",
        "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
        "print(lr_bow_report)\n",
        "\n",
        "#Classification report for tfidf features\n",
        "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
        "print(lr_tfidf_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iecC-bAvJ3XJ"
      },
      "source": [
        "**Confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACAD8qg4J3XK"
      },
      "outputs": [],
      "source": [
        "#confusion matrix for bag of words\n",
        "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
        "print(cm_bow)\n",
        "#confusion matrix for tfidf features\n",
        "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\n",
        "print(cm_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjDcyfF4J3XK"
      },
      "source": [
        "**Stochastic gradient descent or Linear support vector machines for bag of words and tfidf features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxr5BKCzJ3XL"
      },
      "outputs": [],
      "source": [
        "#training the linear svm\n",
        "svm=SGDClassifier(loss='hinge',max_iter=500,random_state=42)\n",
        "#fitting the svm for bag of words\n",
        "svm_bow=svm.fit(cv_train_reviews,train_sentiments)\n",
        "print(svm_bow)\n",
        "#fitting the svm for tfidf features\n",
        "svm_tfidf=svm.fit(tv_train_reviews,train_sentiments)\n",
        "print(svm_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhFibSwUJ3XM"
      },
      "source": [
        "**Model performance on test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeKemU64J3XM"
      },
      "outputs": [],
      "source": [
        "#Predicting the model for bag of words\n",
        "svm_bow_predict=svm.predict(cv_test_reviews)\n",
        "print(svm_bow_predict)\n",
        "#Predicting the model for tfidf features\n",
        "svm_tfidf_predict=svm.predict(tv_test_reviews)\n",
        "print(svm_tfidf_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdEzfRqjJ3XN"
      },
      "source": [
        "**Accuracy of the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvd5x1ZiJ3XN"
      },
      "outputs": [],
      "source": [
        "#Accuracy score for bag of words\n",
        "svm_bow_score=accuracy_score(test_sentiments,svm_bow_predict)\n",
        "print(\"svm_bow_score :\",svm_bow_score)\n",
        "#Accuracy score for tfidf features\n",
        "svm_tfidf_score=accuracy_score(test_sentiments,svm_tfidf_predict)\n",
        "print(\"svm_tfidf_score :\",svm_tfidf_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX86PnPJJ3XO"
      },
      "source": [
        "**Print the classification report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SncOQtWsJ3XO"
      },
      "outputs": [],
      "source": [
        "#Classification report for bag of words\n",
        "svm_bow_report=classification_report(test_sentiments,svm_bow_predict,target_names=['Positive','Negative'])\n",
        "print(svm_bow_report)\n",
        "#Classification report for tfidf features\n",
        "svm_tfidf_report=classification_report(test_sentiments,svm_tfidf_predict,target_names=['Positive','Negative'])\n",
        "print(svm_tfidf_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU7Rh5tkJ3XP"
      },
      "source": [
        "**Plot the confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p7lpc5wJ3XP"
      },
      "outputs": [],
      "source": [
        "#confusion matrix for bag of words\n",
        "cm_bow=confusion_matrix(test_sentiments,svm_bow_predict,labels=[1,0])\n",
        "print(cm_bow)\n",
        "#confusion matrix for tfidf features\n",
        "cm_tfidf=confusion_matrix(test_sentiments,svm_tfidf_predict,labels=[1,0])\n",
        "print(cm_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JqMPqceJ3XQ"
      },
      "source": [
        "**Multinomial Naive Bayes for bag of words and tfidf features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr7Ee5wZJ3XQ"
      },
      "outputs": [],
      "source": [
        "#training the model\n",
        "mnb=MultinomialNB()\n",
        "#fitting the svm for bag of words\n",
        "mnb_bow=mnb.fit(cv_train_reviews,train_sentiments)\n",
        "print(mnb_bow)\n",
        "#fitting the svm for tfidf features\n",
        "mnb_tfidf=mnb.fit(tv_train_reviews,train_sentiments)\n",
        "print(mnb_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYCryk1xJ3XR"
      },
      "source": [
        "**Model performance on test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49_CRkRgJ3XS"
      },
      "outputs": [],
      "source": [
        "#Predicting the model for bag of words\n",
        "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
        "print(mnb_bow_predict)\n",
        "#Predicting the model for tfidf features\n",
        "mnb_tfidf_predict=mnb.predict(tv_test_reviews)\n",
        "print(mnb_tfidf_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQmtUk65J3XS"
      },
      "source": [
        "**Accuracy of the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4jSxKxZJ3XT"
      },
      "outputs": [],
      "source": [
        "#Accuracy score for bag of words\n",
        "mnb_bow_score=accuracy_score(test_sentiments,mnb_bow_predict)\n",
        "print(\"mnb_bow_score :\",mnb_bow_score)\n",
        "#Accuracy score for tfidf features\n",
        "mnb_tfidf_score=accuracy_score(test_sentiments,mnb_tfidf_predict)\n",
        "print(\"mnb_tfidf_score :\",mnb_tfidf_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o125HblYJ3XV"
      },
      "source": [
        "**Print the classification report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_G-ISBYJ3XW"
      },
      "outputs": [],
      "source": [
        "#Classification report for bag of words\n",
        "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
        "print(mnb_bow_report)\n",
        "#Classification report for tfidf features\n",
        "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
        "print(mnb_tfidf_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztdhUe_AJ3XW"
      },
      "source": [
        "**Plot the confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-ki8evpJ3XX"
      },
      "outputs": [],
      "source": [
        "#confusion matrix for bag of words\n",
        "cm_bow=confusion_matrix(test_sentiments,mnb_bow_predict,labels=[1,0])\n",
        "print(cm_bow)\n",
        "#confusion matrix for tfidf features\n",
        "cm_tfidf=confusion_matrix(test_sentiments,mnb_tfidf_predict,labels=[1,0])\n",
        "print(cm_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYP1GWBaJ3XX"
      },
      "source": [
        "**Let us see positive and negative words by using WordCloud.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjeoiTx6J3XY"
      },
      "source": [
        "**Word cloud for positive review words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9paTydCIJ3XY"
      },
      "outputs": [],
      "source": [
        "#word cloud for positive review words\n",
        "plt.figure(figsize=(10,10))\n",
        "positive_text=norm_train_reviews[1]\n",
        "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
        "positive_words=WC.generate(positive_text)\n",
        "plt.imshow(positive_words,interpolation='bilinear')\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhvR42y1J3XZ"
      },
      "source": [
        "**Word cloud for negative review words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsPF6bK9J3XZ"
      },
      "outputs": [],
      "source": [
        "#Word cloud for negative review words\n",
        "plt.figure(figsize=(10,10))\n",
        "negative_text=norm_train_reviews[8]\n",
        "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
        "negative_words=WC.generate(negative_text)\n",
        "plt.imshow(negative_words,interpolation='bilinear')\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BriEcJ3TJ3Xa"
      },
      "source": [
        "**Conclusion:**\n",
        "* We can observed that both logistic regression and multinomial naive bayes model performing well compared to linear support vector  machines.\n",
        "* Still we can improve the accuracy of the models by preprocessing data and by using lexicon models like Textblob."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}